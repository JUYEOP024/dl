{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47d7a2be",
   "metadata": {},
   "source": [
    "## Pytorch autograd (자동 미분)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46364c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2894831e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 역전파를 위해서 X에 대한 gradient 계산 형성\n",
    "x = torch.FloatTensor([[1, 2],\n",
    "                        [3, 4]]).requires_grad_(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69cd1b61",
   "metadata": {},
   "source": [
    "requires_grad_(True) : 이 텐서에 대해서 미분값을 구하겠다.   \n",
    "일반적인 학습에서는 필요없지만 입력에 대한 기울기 분석시에 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5a15ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 4.],\n",
      "        [5., 6.]], grad_fn=<AddBackward0>)\n",
      "tensor([[-1.,  0.],\n",
      "        [ 1.,  2.]], grad_fn=<SubBackward0>)\n",
      "tensor([[-3.,  0.],\n",
      "        [ 5., 12.]], grad_fn=<MulBackward0>)\n",
      "tensor(14., grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 연산트리 자동생성 \n",
    "x1 = x + 2      # 모든 원소에 +2\n",
    "x2 = x - 2      # 모든 원소에 -2\n",
    "x3 = x1 * x2    # (x+2) 와 (x-2) 텐서를 원소별로 곱함\n",
    "y = x3.sum()    # 모든 원소를 더해 스칼라값으로 변환\n",
    "\n",
    "print(x1)\n",
    "print(x2)\n",
    "print(x3)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a88b9df",
   "metadata": {},
   "source": [
    "### grad_fn 이 각 연산을 추적"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e88b3596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 자동 미분을 통한 역전파 \n",
    "y.backward()    # 스칼라값 y를 기준으로 연결된 모든 텐서의 gradient를 자동으로 계산 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7b846a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 4.],\n",
      "        [6., 8.]])\n"
     ]
    }
   ],
   "source": [
    "# 역전파 이후 x 에 대한 손실함수의 기울기(미분값) 확인\n",
    "print(x.grad) # y에 대해 계산된 x의 gradient(∂y/∂x) 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55d16fe",
   "metadata": {},
   "source": [
    "x =    \n",
    "tensor[[1, 2],   \n",
    "        [3, 4]]   \n",
    "\n",
    "=> 편미분값  \n",
    "tensor([[2., 4.],  \n",
    "        [6., 8.]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c90c808",
   "metadata": {},
   "source": [
    "y = Σ(x+2)(x-2)   \n",
    "=>\n",
    "Σ($x^2$ - 4)\n",
    "각 원소에 대해 $x^2$ - 4를 계산한 후에 전부 더한 것\n",
    "\n",
    "- 상수를 미분하면?\n",
    "$\\frac{\\partial}{\\partial x}(-4)$ = 0\n",
    "=> -4는 미분에 영향을 주지 않는다.\n",
    "- 제곱항을 주게되면?\n",
    "$\\frac{\\partial}{\\partial x_i}(x_i^2) = 2x_i$ \n",
    "\n",
    "- 각 원소에 대해서 결과는\n",
    "$\\frac{\\partial y}{\\partial x_i} = 2x_i$\n",
    "\n",
    "- 이걸 벡터화시키게 되면 \n",
    "$\\frac{\\partial y}{\\partial x} = 2x$\n",
    "\n",
    "=> x.grad 출력 시 각 원소에 대해 x2값을 가지게 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b475560a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 4.]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(x) # 텐서값은 변하지않음. 역전파 시 기울기는 계산하지만 값을 수정하지는 않는다\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2558e8",
   "metadata": {},
   "source": [
    "실제 값을 수정할때는    optimizer.step()   \n",
    "최적화함수 사용하여 가중치 수정한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d949381",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mx3\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead."
     ]
    }
   ],
   "source": [
    "x3.numpy() # RuntimeError # 계산 그래프가 연결된 텐서는 numpy변환을 직접 허용하지 않는다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dcc85d5",
   "metadata": {},
   "source": [
    "numpy는 autograd를 모르기때문에 허용하지 않는다.  \n",
    "즉 미분 추적을 깨트릴 수 있기 때문에(무결성)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a18788a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3.,  0.],\n",
       "       [ 5., 12.]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x3.detach_().numpy() # detach_ == inplace 로 계산 그래프 분리 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc48c03",
   "metadata": {},
   "source": [
    "- detach 와 detach_\n",
    "\n",
    "| 구분 | detach() | detach_() |\n",
    "|-----|---------|-----------|\n",
    "| 동작 방식 | 복사본 만들어 분리 | 원본 자체를 분리 |\n",
    "| 원본 손상 여부 | 손상 없음 | 원본이 autograd 추적 불가 상태로 바뀐다 |\n",
    "| 사용 시점 | 일반적으로 사용 | 성능이 매우 중요할 때(추적 불가 - 주의해서 사용) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d1a266e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-3.,  0.],\n",
      "        [ 5., 12.]], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.FloatTensor([[1, 2],\n",
    "                        [3, 4]]).requires_grad_(True)\n",
    "x1 = x + 2      # 모든 원소에 +2\n",
    "x2 = x - 2      # 모든 원소에 -2\n",
    "x3 = x1 * x2    # (x+2) 와 (x-2) 텐서를 원소별로 곱함\n",
    "\n",
    "print(x3) # autograd 연결 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fe78ea64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-3.,  0.],\n",
      "        [ 5., 12.]])\n"
     ]
    }
   ],
   "source": [
    "x3.detach_()\n",
    "\n",
    "print(x3) # autograd 연결 해제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea01c8c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<built-in method numpy of Tensor object at 0x10f5b5810>\n"
     ]
    }
   ],
   "source": [
    "print(x3.numpy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
