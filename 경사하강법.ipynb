{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e5eb9793",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9b58fe82",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = torch.FloatTensor([[.1, .2, .3],\n",
    "                             [.4, .5, .6],\n",
    "                             [.7, .8, .9]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ca2f81e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3663, 0.5940, 0.6815],\n",
       "        [0.6202, 0.7916, 0.8917],\n",
       "        [0.2463, 0.7045, 0.9785]], requires_grad=True)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand_like(target) # 0 ~ 1 범위에서 랜덤값으로 target 과 같은 shape의 텐서를 생성\n",
    "x.requires_grad = True      # x 를 미분 가능한 텐서로 설정하여. backward() 호출 시 x에 대한 기울기 자동 계산 \n",
    "\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "956caebc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0902, grad_fn=<MseLossBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = F.mse_loss(x, target) # x와 target간의 평균 제곱오차(MSE) 계산 (x는 예측값, target은 정답값으로 가정)\n",
    "\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9360750f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1번째 Loss: 0.054543547332286835\n",
      "tensor([[0.3072, 0.5064, 0.5967],\n",
      "        [0.5713, 0.7268, 0.8269],\n",
      "        [0.3471, 0.7257, 0.9610]], requires_grad=True)\n",
      "2번째 Loss: 0.03299547731876373\n",
      "tensor([[0.2611, 0.4383, 0.5308],\n",
      "        [0.5332, 0.6764, 0.7765],\n",
      "        [0.4255, 0.7422, 0.9475]], requires_grad=True)\n",
      "3번째 Loss: 0.019960228353738785\n",
      "tensor([[0.2253, 0.3854, 0.4795],\n",
      "        [0.5036, 0.6372, 0.7373],\n",
      "        [0.4865, 0.7551, 0.9369]], requires_grad=True)\n",
      "4번째 Loss: 0.01207470428198576\n",
      "tensor([[0.1975, 0.3442, 0.4396],\n",
      "        [0.4806, 0.6067, 0.7068],\n",
      "        [0.5340, 0.7651, 0.9287]], requires_grad=True)\n",
      "5번째 Loss: 0.007304450497031212\n",
      "tensor([[0.1758, 0.3121, 0.4086],\n",
      "        [0.4627, 0.5830, 0.6830],\n",
      "        [0.5709, 0.7728, 0.9223]], requires_grad=True)\n",
      "6번째 Loss: 0.004418742377310991\n",
      "tensor([[0.1590, 0.2872, 0.3844],\n",
      "        [0.4488, 0.5646, 0.6646],\n",
      "        [0.5996, 0.7789, 0.9174]], requires_grad=True)\n",
      "7번째 Loss: 0.002673066221177578\n",
      "tensor([[0.1459, 0.2678, 0.3657],\n",
      "        [0.4379, 0.5502, 0.6502],\n",
      "        [0.6219, 0.7836, 0.9135]], requires_grad=True)\n",
      "8번째 Loss: 0.0016170400194823742\n",
      "tensor([[0.1357, 0.2528, 0.3511],\n",
      "        [0.4295, 0.5391, 0.6391],\n",
      "        [0.6392, 0.7872, 0.9105]], requires_grad=True)\n",
      "9번째 Loss: 0.0009782095439732075\n",
      "tensor([[0.1277, 0.2410, 0.3397],\n",
      "        [0.4229, 0.5304, 0.6304],\n",
      "        [0.6527, 0.7901, 0.9082]], requires_grad=True)\n",
      "10번째 Loss: 0.0005917565431445837\n",
      "tensor([[0.1216, 0.2319, 0.3309],\n",
      "        [0.4178, 0.5236, 0.6236],\n",
      "        [0.6632, 0.7923, 0.9064]], requires_grad=True)\n",
      "11번째 Loss: 0.0003579759504646063\n",
      "tensor([[0.1168, 0.2248, 0.3240],\n",
      "        [0.4139, 0.5184, 0.6184],\n",
      "        [0.6714, 0.7940, 0.9049]], requires_grad=True)\n",
      "12번째 Loss: 0.00021655358432326466\n",
      "tensor([[0.1131, 0.2193, 0.3187],\n",
      "        [0.4108, 0.5143, 0.6143],\n",
      "        [0.6778, 0.7953, 0.9038]], requires_grad=True)\n",
      "13번째 Loss: 0.00013100137584842741\n",
      "tensor([[0.1102, 0.2150, 0.3145],\n",
      "        [0.4084, 0.5111, 0.6111],\n",
      "        [0.6827, 0.7964, 0.9030]], requires_grad=True)\n",
      "14번째 Loss: 7.924781675683334e-05\n",
      "tensor([[0.1079, 0.2117, 0.3113],\n",
      "        [0.4065, 0.5086, 0.6086],\n",
      "        [0.6865, 0.7972, 0.9023]], requires_grad=True)\n",
      "15번째 Loss: 4.794002234120853e-05\n",
      "tensor([[0.1061, 0.2091, 0.3088],\n",
      "        [0.4051, 0.5067, 0.6067],\n",
      "        [0.6895, 0.7978, 0.9018]], requires_grad=True)\n",
      "16번째 Loss: 2.9000735594308935e-05\n",
      "tensor([[0.1048, 0.2071, 0.3068],\n",
      "        [0.4040, 0.5052, 0.6052],\n",
      "        [0.6919, 0.7983, 0.9014]], requires_grad=True)\n",
      "17번째 Loss: 1.7543618014315143e-05\n",
      "tensor([[0.1037, 0.2055, 0.3053],\n",
      "        [0.4031, 0.5041, 0.6041],\n",
      "        [0.6937, 0.7987, 0.9011]], requires_grad=True)\n",
      "18번째 Loss: 1.061280181602342e-05\n",
      "tensor([[0.1029, 0.2043, 0.3041],\n",
      "        [0.4024, 0.5032, 0.6032],\n",
      "        [0.6951, 0.7990, 0.9009]], requires_grad=True)\n",
      "19번째 Loss: 6.420119916583644e-06\n",
      "tensor([[0.1022, 0.2033, 0.3032],\n",
      "        [0.4019, 0.5025, 0.6025],\n",
      "        [0.6962, 0.7992, 0.9007]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "threshold = 1e-5        # 손실함수 종료 기준값\n",
    "learning_rate = 1.      # 가중치 업데이트에 사용할 학습률\n",
    "iter_cnt = 0            # 반복 횟수 카운터 초기화\n",
    "\n",
    "while loss > threshold:                     # 손실이 임계값보다 클 동안 반복\n",
    "    iter_cnt += 1                           # 반복 횟수 증가\n",
    "\n",
    "    loss.backward()                         # loss에 대한 역전파 수행 (∂loss / ∂x 계산)\n",
    "\n",
    "    x = x - learning_rate * x.grad          # gradient descent 방식으로 x를 업데이트\n",
    "\n",
    "    x.detach_()                             # 기존 계산 그래프에서 x 분리 \n",
    "    x.requires_grad_(True)                  # 다음 반복을 위해 gradient 추적 다시 활성화\n",
    "\n",
    "    loss = F.mse_loss(x, target)            # 업데이트된 x로 새로운 손실 계산\n",
    "\n",
    "    print(f\"{iter_cnt}번째 Loss: {loss}\")    # 현재 반복의 손실 값 출력\n",
    "    print(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
